{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Understanding LLMs and Pre-training\n",
    "\n",
    "In this tutorial, we will explore the mechanics of LLM architectures, with an emphasis on the differences between masked models and causal models. In the first section, we'll examine some existing pretrained models to understand how they produce their outputs. Once we've demonstrated how LLM's are able to do what they do, we will then run an abbreviated training loop to provide a glimpse into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p66qIkzd7wz"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zGW64FX-d7wz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[sentencepiece,torch]\n",
      "\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/grpcio-1.69.0-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/google_resumable_media-2.7.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/Deprecated-1.2.15-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/shapely-2.0.6-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/configloader-1.0.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/importlib_metadata-6.11.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/bluerock-0.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/enum34-1.1.10-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/proto_plus-1.25.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/recommendation-1.0.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/fire-0.7.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/elasticsearch-8.17.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/python_docx-1.1.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/pyclipper-1.3.0.post6-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/google_cloud_core-2.4.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/vnpy_ib-10.19.1.10-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/vnpy_ibdata-1.0.0.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/s3transfer-0.11.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/pyasn1_modules-0.4.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/urllib3-1.26.2-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/grpcio_tools-1.69.0-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/elastic_transport-8.17.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/albumentations-1.4.10-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/google_cloud_storage-3.0.0rc1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/googleapis_common_protos-1.66.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/scikit_image-0.25.0-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/google-3.0.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/Django-3.2.25-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/albucore-0.0.13-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/jmespath-1.0.1-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/google_auth-2.37.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/argparse-1.4.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/opencv_contrib_python-4.11.0.86-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/zipfile38-0.0.3-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/Backtesting-0.1.dev283+g066a58a-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/kullm-1.0.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/protobuf-5.29.3-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/paddleocr-0.0.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/opencv_python_headless-4.11.0.86-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mDEPRECATION: Loading egg at /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/lmdb-1.6.2-py3.12-macosx-11.0-arm64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001B[0m\u001B[33m\n",
      "\u001B[0mCollecting apache_beam\n",
      "  Downloading apache_beam-2.64.0.tar.gz (2.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.6/2.6 MB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting crcmod<2.0,>=1.7 (from apache_beam)\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: orjson<4,>=3.9.7 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (3.10.4)\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache_beam)\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting cloudpickle~=2.2.1 (from apache_beam)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache_beam)\n",
      "  Downloading fastavro-1.10.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (5.5 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache_beam)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache_beam)\n",
      "  Downloading grpcio-1.65.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache_beam)\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: httplib2<0.23.0,>=0.8 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (0.22.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (4.19.2)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache_beam)\n",
      "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=1.14.3 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (1.26.4)\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache_beam)\n",
      "  Downloading objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (24.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (4.10.1)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/proto_plus-1.25.0-py3.12.egg (from apache_beam) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/protobuf-5.29.3-py3.12.egg (from apache_beam) (5.29.3)\n",
      "Collecting pydot<2,>=1.2.0 (from apache_beam)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (2024.2)\n",
      "Requirement already satisfied: redis<6,>=5.0.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (5.0.6)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (2024.9.11)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (2.32.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (4.13.0)\n",
      "Collecting zstandard<1,>=0.18.0 (from apache_beam)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from apache_beam) (6.0.1)\n",
      "Collecting pyarrow<17.0.0,>=3.0.0 (from apache_beam)\n",
      "  Downloading pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix<1 (from apache_beam)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache_beam)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from httplib2<0.23.0,>=0.8->apache_beam) (3.2.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.10.6)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from pymongo<5.0.0,>=3.8.0->apache_beam) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages/urllib3-1.26.2-py3.12.egg (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashokpant/miniconda3/envs/ml/lib/python3.12/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2024.7.4)\n",
      "Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading fastavro-1.10.0-cp312-cp312-macosx_10_13_universal2.whl (1.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.65.5-cp312-cp312-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.4/10.4 MB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\n",
      "\u001B[?25hDownloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
      "Downloading objsize-0.7.1-py3-none-any.whl (11 kB)\n",
      "Downloading pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl (26.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.0/26.0 MB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m633.5/633.5 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: apache_beam, crcmod, dill, hdfs\n",
      "  Building wheel for apache_beam (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for apache_beam: filename=apache_beam-2.64.0-cp312-cp312-macosx_11_0_arm64.whl size=5494114 sha256=de4d1d6533e381f95920b274adf97541bb2284aa6759787f34665ef1f601c620\n",
      "  Stored in directory: /Users/ashokpant/Library/Caches/pip/wheels/24/56/cf/b98795275cf269fbc6aeb07bc727f96e6ace62daadc47bcdd1\n",
      "  Building wheel for crcmod (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp312-cp312-macosx_11_0_arm64.whl size=21766 sha256=e57d2938a35bc171f372a913900528d13bcbcc679e7c12d08ec8aaeee5367c09\n",
      "  Stored in directory: /Users/ashokpant/Library/Caches/pip/wheels/76/08/0b/caa8b1380122cbfe6a03eaccbec0f63c67e619af4e30ca5e2a\n",
      "  Building wheel for dill (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78634 sha256=0d292a6115953c5821e26e30a94eee4591b94a8a28f7f3e57ed879ff1a76bdc3\n",
      "  Stored in directory: /Users/ashokpant/Library/Caches/pip/wheels/84/0e/3b/161164c3e023dd2f01045d3848c0f5f96f4dd9a058a2cd300b\n",
      "  Building wheel for hdfs (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34431 sha256=fa6781d732e46eee8dde7adf307f454a56ea97c72d0c49c6cecf58004583bb4e\n",
      "  Stored in directory: /Users/ashokpant/Library/Caches/pip/wheels/97/ae/d9/536505928dd3a458b206013b02625df8f12d22fa154f2bfd65\n",
      "Successfully built apache_beam crcmod dill hdfs\n",
      "Installing collected packages: docopt, crcmod, zstandard, pydot, pyarrow-hotfix, pyarrow, objsize, jsonpickle, grpcio, fasteners, fastavro, dill, cloudpickle, hdfs, apache_beam\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.70.0\n",
      "    Uninstalling grpcio-1.70.0:\n",
      "      Successfully uninstalled grpcio-1.70.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 3.0.0\n",
      "    Uninstalling cloudpickle-3.0.0:\n",
      "      Successfully uninstalled cloudpickle-3.0.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pycaret 3.3.2 requires joblib<1.4,>=1.2.0, but you have joblib 1.4.2 which is incompatible.\n",
      "pycaret 3.3.2 requires matplotlib<3.8.0, but you have matplotlib 3.10.0 which is incompatible.\n",
      "pycaret 3.3.2 requires pandas<2.2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "pycaret 3.3.2 requires scipy<=1.11.4,>=1.6.1, but you have scipy 1.13.1 which is incompatible.\n",
      "dask 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "grpcio-status 1.70.0 requires grpcio>=1.70.0, but you have grpcio 1.65.5 which is incompatible.\n",
      "tensorflow 2.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
      "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.16.2 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.1.0 which is incompatible.\n",
      "distributed 2024.10.0 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\n",
      "distributed 2024.10.0 requires urllib3>=1.26.5, but you have urllib3 1.26.2 which is incompatible.\n",
      "treeutil 1.0.0 requires grpcio<1.70.0,>=1.69.0, but you have grpcio 1.65.5 which is incompatible.\n",
      "treeutil 1.0.0 requires numpy<2.3.0,>=2.2.2, but you have numpy 1.26.4 which is incompatible.\n",
      "treeutil 1.0.0 requires opentelemetry-api<1.30.0,>=1.29.0, but you have opentelemetry-api 1.30.0 which is incompatible.\n",
      "treeutil 1.0.0 requires opentelemetry-instrumentation<0.51,>=0.50b0, but you have opentelemetry-instrumentation 0.51b0 which is incompatible.\n",
      "treeutil 1.0.0 requires opentelemetry-sdk<1.30.0,>=1.29.0, but you have opentelemetry-sdk 1.30.0 which is incompatible.\n",
      "treeutil 1.0.0 requires setuptools<75.9.0,>=75.8.0, but you have setuptools 78.1.0 which is incompatible.\n",
      "treeutil 1.0.0 requires urllib3<2.3.0,>=2.2.2, but you have urllib3 1.26.2 which is incompatible.\n",
      "faceattandance 1.0.0 requires grpcio<1.70.0,>=1.69.0, but you have grpcio 1.65.5 which is incompatible.\n",
      "faceattandance 1.0.0 requires numpy<2.3.0,>=2.2.2, but you have numpy 1.26.4 which is incompatible.\n",
      "faceattandance 1.0.0 requires scipy<1.16.0,>=1.15.1, but you have scipy 1.13.1 which is incompatible.\n",
      "faceattandance 1.0.0 requires torch<2.6.0,>=2.5.1, but you have torch 2.6.0 which is incompatible.\n",
      "faceattandance 1.0.0 requires urllib3<2.3.0,>=2.2.0, but you have urllib3 1.26.2 which is incompatible.\n",
      "nautilus-trader 1.207.0 requires pyarrow>=18.1.0, but you have pyarrow 16.1.0 which is incompatible.\n",
      "grpcio-tools 1.69.0 requires grpcio>=1.69.0, but you have grpcio 1.65.5 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed apache_beam-2.64.0 cloudpickle-2.2.1 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.10.0 fasteners-0.19 grpcio-1.69.0 hdfs-2.7.3 jsonpickle-3.4.2 objsize-0.7.1 pyarrow-16.1.0 pyarrow-hotfix-0.6 pydot-1.4.2 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers[sentencepiece,torch]\n",
    "!pip install apache_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGc8Si8CBbuI"
   },
   "source": [
    "## Understanding Masked LM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "99o28B2EGVh3"
   },
   "outputs": [],
   "source": [
    "## The first model we will look at is BERT, which is trained with masked tokens. As an example,\n",
    "## the text below masks the word \"box\" from a well-known movie quote.\n",
    "\n",
    "text = \"Life is like a [MASK] of chocolates.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZVii0JgSBKpU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## We'll now see how BERT is able to predict the missing word. We can use HuggingFace to load\n",
    "## a copy of the pretrained model and tokenizer.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TWKpRFEFA2Oz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 101, 2166, 2003, 2066, 1037,  103, 1997, 7967, 2015, 1012,  102]])\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## Next, we'll feed our example text into the tokenizer.\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print('input_ids:', encoded_input['input_ids'])\n",
    "print('attention_mask:', encoded_input['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o84muqUuLiwC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate\n"
     ]
    }
   ],
   "source": [
    "## input_ids represents the tokenized output. Each integer can be mapped back to the corresponding string.\n",
    "\n",
    "print(tokenizer.decode([7967]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1ut5R3IYbxxd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The model will then receive the output of the tokenizer. We can look at the BERT model to see exactly how\n",
    "## it was constructed and what the outputs will be like.\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H-Xv2ujRAch4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 30522])\n"
     ]
    }
   ],
   "source": [
    "## The model starts with an embedding of each of the 30,522 possible tokens into 768 dimensions, which at this\n",
    "## point is simply a representation of each token without any additional information about their relationships\n",
    "## to one another in the text. Then the encoder attention blocks are applied, updating the embeddings such that\n",
    "## they now encode each token's contribution to the chunk of text and interactions with other tokens. Notably,\n",
    "## this includes the masked tokens as well. The final stage is the language model head, which takes the embeddings\n",
    "## from the masked positions back to 30,522 dimensions. Each index of this final vector corresponds to the\n",
    "## probability that the token in that position would be the correct choice to fill the mask.\n",
    "\n",
    "\n",
    "model_output = model(**encoded_input)\n",
    "output = model_output[\"logits\"]\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4hDXB-x5tzrh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "tokens = encoded_input['input_ids'][0].tolist()\n",
    "masked_index = tokens.index(tokenizer.mask_token_id)\n",
    "logits = output[0, masked_index, :]\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BaRYd_aVjeoR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions: box bag bowl jar cup\n",
      "tensor([0.1764, 0.1688, 0.0419, 0.0336, 0.0262], grad_fn=<TopkBackward0>)\n"
     ]
    }
   ],
   "source": [
    "probs = logits.softmax(dim=-1)\n",
    "values, predictions = probs.topk(5)\n",
    "sequence = tokenizer.decode(predictions)\n",
    "\n",
    "print('Top 5 predictions:', sequence)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vVQ6Cqhuq9t"
   },
   "source": [
    "Printing the top 5 predictions and their respective scores, we see that BERT accurately chooses \"box\" as the most likely replacement for the mask token.\n",
    "\n",
    "## Understanding Causal LM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "azFAE8TVkPXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf95ca83b0f43ed98d72fb4b49b0d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1ce119a17540baadb7f8c1d4872646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## We now repeat a similar exercise with the causal LLM GPT-2. This model generates\n",
    "## text following an input, instead of replacing a mask within the text.\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4TwmbE0bk1mC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can examine the model again, noting the similarities to BERT. An embedding, 12 attention blocks,\n",
    "## and a linear transformation bringing the output back to the size of the tokenizer. The tokenizer is\n",
    "## different from BERT so we see we have more tokens this time.\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hwUCWU6TwTqZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We'll use a different text example, since this model works by producing tokens sequentially\n",
    "## rather than filling a mask.\n",
    "\n",
    "text = \"Swimming at the beach is\"\n",
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ua8YctwekqSm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([257])\n"
     ]
    }
   ],
   "source": [
    "## After applying the model, the information needed to predict the next token is represented by\n",
    "## the last token. So we can access that vector by the index -1.\n",
    "\n",
    "output = model(**model_inputs)\n",
    "next_token_logits = output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OH05O28PnLW9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "## Now add the new token to the end of the text, and feed all of it back to the model to continue\n",
    "## predicting more tokens.\n",
    "\n",
    "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
    "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-ja9Mk0DnpBe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a\n"
     ]
    }
   ],
   "source": [
    "## Here's what we have so far. The model added the word 'a' to the input text.\n",
    "\n",
    "print(tokenizer.decode(model_inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "n-ygamSIoMKh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swimming at the beach is a great\n"
     ]
    }
   ],
   "source": [
    "## Repeating all the previous steps, we then add the word 'great'.\n",
    "\n",
    "output = model(**model_inputs)\n",
    "next_token_logits = output.logits[:, -1, :]\n",
    "next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
    "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
    "print(tokenizer.decode(model_inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9QIw--gIrh8"
   },
   "outputs": [],
   "source": [
    "## HuggingFace automates this iterative process. We'll use the quicker approach to finish our sentence.\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output_generate[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XL2rmSMVpkEu"
   },
   "source": [
    "## Pre-training a GPT-2 model from scratch\n",
    "\n",
    "Next we'll train a GPT-2 model from scratch using English Wikipedia data. Note that we're only using a tiny subset of the data to demonstrate that the model is capable of learning. The exact same approach could be followed on the full dataset to train a more functional model, but that would require a lot of compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTW0UmOJd7w2"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "ds_shuffle = dataset['train'].shuffle()\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_shuffle.select(range(50)),\n",
    "        \"valid\": ds_shuffle.select(range(50, 100))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qbgOvH3d7w2"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets['train'][0]['text'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HRmdiqbd7w3"
   },
   "outputs": [],
   "source": [
    "## We'll tokenize the text, setting the context size to 128 and thus breaking each document into chunks of 128 tokens.\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWvoQ3-td7w3"
   },
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfu1oJeQYiLa"
   },
   "source": [
    "Now we can set up the HuggingFace Trainer as follows. Since we're using such a small dataset, we'll need lots of epochs for the model to make progress because all of the parameters are randomly initialized at the outset. Typically, most LLM's are trained for only one epoch and more diverse examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jf0Le4Yd7w3"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_OKDaPMd7w3"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex3MHkued7w4"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"wiki-gpt2\",\n",
    "    eval_strategy=\"steps\",\n",
    "    num_train_epochs=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIj0n1fbd7w4"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2avxzYR5U8Y"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIkCLEFRc3h6"
   },
   "source": [
    "The training loss is low by the end, which means the model should perform very well on training examples it has seen. It does not generalize well to the validation set of course, since we deliberately overfit on a small train set.\n",
    "\n",
    "We can confirm with a couple of examples that were seen in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-cxmRLLsGgB"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'][:16])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbk_WBhMrj_F"
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "print(model_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipgkkqn4sEAp"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
    "output_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHUbsd-hxdTL"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.decode(output_generate[0])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l04YrQLeghA5"
   },
   "source": [
    "The model should do quite well at reciting text after seeing it so many times. We can be convinced that the tokenizer, model architecture, and training objective are well-suited to learning Wikipedia data. For comparison, we'll try this model on text from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFK5FC57hwS9"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:32])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SlX_sfHfdTB"
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
    "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
    "\n",
    "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
    "sequence = tokenizer.decode(output_generate[0])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FassINSdfs9U"
   },
   "outputs": [],
   "source": [
    "raw_datasets['valid'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ivQP0XciLl1"
   },
   "source": [
    "As expected, our model is completely confused this time. We'd need to train for much longer, and on much more diverse data, before we would have a model that can sensibly complete prompts it has never seen before. This is precisely why pre-training is such an important and powerful technique. If we had to train on all of Wikipedia for every NLP application to achieve optimal performance, it would be prohibitively expensive. But there's no need to do that when we can share and reuse existing pre-trained models as we did in the first part of this tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
