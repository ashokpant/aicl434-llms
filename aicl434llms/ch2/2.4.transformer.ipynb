{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7f4c0ced19964e",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation\n",
    "\n",
    "References\n",
    "1. https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c9ca775f9bab94",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Self-Attention\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(x.shape[-1])\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(weights, V)\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.heads = nn.ModuleList([SelfAttention(self.d_k) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(self.num_heads, dim=-1)\n",
    "        attended = [head(chunk) for head, chunk in zip(self.heads, chunks)]\n",
    "        concat = torch.cat(attended, dim=-1)\n",
    "        return self.linear(concat)\n",
    "\n",
    "# Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attn(x))\n",
    "        return self.norm2(x + self.ff(x))\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out):\n",
    "        x = self.norm1(x + self.self_attn(x))\n",
    "        x = self.norm2(x + self.cross_attn(x))\n",
    "        return self.norm3(x + self.ff(x))\n",
    "\n",
    "# Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional = PositionalEncoding(d_model)\n",
    "        self.encoder = EncoderLayer(d_model, num_heads, d_ff)\n",
    "        self.decoder = DecoderLayer(d_model, num_heads, d_ff)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.positional(self.embedding(src))\n",
    "        tgt = self.positional(self.embedding(tgt))\n",
    "        enc_out = self.encoder(src)\n",
    "        dec_out = self.decoder(tgt, enc_out)\n",
    "        return self.output(dec_out)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt_in, tgt_out in dataloader:\n",
    "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "            output = model(src, tgt_in)\n",
    "            pred = output.argmax(dim=-1)\n",
    "            correct += (pred == tgt_out).sum().item()\n",
    "            total += tgt_out.numel()\n",
    "    return correct / total\n",
    "\n",
    "# Inference\n",
    "def predict(model, src_seq, max_len=4, bos_token=1):\n",
    "    model.eval()\n",
    "    src_seq = torch.tensor(src_seq).unsqueeze(0).to(next(model.parameters()).device)\n",
    "    encoder_input = model.positional(model.embedding(src_seq))\n",
    "    enc_output = model.encoder(encoder_input)\n",
    "    tgt_seq = torch.tensor([[bos_token]]).to(src_seq.device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        tgt_embed = model.positional(model.embedding(tgt_seq))\n",
    "        dec_output = model.decoder(tgt_embed, enc_output)\n",
    "        logits = model.output(dec_output)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        tgt_seq = torch.cat([tgt_seq, next_token], dim=1)\n",
    "    return tgt_seq.squeeze(0).tolist()\n",
    "\n",
    "# Dummy Seq2Seq Dataset\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, seq_len=4, vocab_size=15):\n",
    "        self.data = []\n",
    "     \n",
    "        for _ in range(num_samples):\n",
    "            src = [random.randint(2, vocab_size - 1) for _ in range(seq_len)]\n",
    "            tgt = [1] + src  # 1 = BOS\n",
    "            self.data.append((src, tgt))\n",
    "     \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        return torch.tensor(src), torch.tensor(tgt[:-1]), torch.tensor(tgt[1:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "# Training\n",
    "def train_model():\n",
    "    d_model = 16\n",
    "    num_heads = 2\n",
    "    d_ff = 32\n",
    "    vocab_size = 15\n",
    "    epochs = 5\n",
    "    batch_size = 8\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = Seq2SeqDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = Transformer(d_model, num_heads, d_ff, vocab_size).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src, tgt_in, tgt_out in dataloader:\n",
    "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_in)\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt_out.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        acc = evaluate(model, dataloader, device)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ffc867f-6e92-4735-8171-a451f7ef55ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset Example\n",
    "# dataset = Seq2SeqDataset()\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "# for src, tgt_in, tgt_out in dataloader:\n",
    "#     print(\"Sample\", \"src\", src, \"tgt_in\", tgt_in, \"tgt_out\", tgt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "315cfff096289f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 36.3839, Accuracy: 0.0850\n",
      "Epoch 2, Loss: 35.2181, Accuracy: 0.0850\n",
      "Epoch 3, Loss: 34.3994, Accuracy: 0.1025\n",
      "Epoch 4, Loss: 33.9681, Accuracy: 0.1375\n",
      "Epoch 5, Loss: 33.5007, Accuracy: 0.1750\n",
      "Input:      [2, 2, 3, 4]\n",
      "Predicted:  [1, 13, 6, 6, 13]\n"
     ]
    }
   ],
   "source": [
    "# Run training and prediction\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_model()\n",
    "    test_input = [2, 2, 3, 4]\n",
    "    prediction = predict(trained_model, test_input)\n",
    "    print(\"Input:     \", test_input)\n",
    "    print(\"Predicted: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01187f2e-35ea-48cd-b1ce-9dafddd4de12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
