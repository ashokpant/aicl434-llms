{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3279b95fde2e367f",
   "metadata": {},
   "source": [
    "# Tokenizer Example\n",
    "Ashok Kumar Pant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0375c823413e399",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 19204, 17629, 2015, 10463, 2616, 2000, 3616, 999, 102]\n",
      "Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['[CLS]', 'token', '##izer', '##s', 'convert', 'words', 'to', 'numbers', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained tokenizer (e.g., BERT tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Tokenizers convert words to numbers!\"\n",
    "\n",
    "# Tokenize the input\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "print(\"Input IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Token Type IDs:\", tokens[\"token_type_ids\"])\n",
    "print(\"Attention Mask:\", tokens[\"attention_mask\"])\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ea98131acc1cb",
   "metadata": {},
   "source": [
    "## Explanation of the Output\n",
    "#### Tokens: ['[CLS]', 'token', '##izer', '##s', 'convert', 'words', 'to', 'numbers', '!', '[SEP]']\n",
    "These are the subword tokens produced by the tokenizer (in this case, BERT's tokenizer).\n",
    "- '[CLS]' — Special classification token added at the beginning of the input (used by BERT for classification tasks).\n",
    "- 'token', '##izer', '##s' — BERT uses WordPiece tokenization, which breaks unfamiliar or compound words into smaller known subwords.\n",
    "- 'tokenizers' was split into ['token', '##izer', '##s'].\n",
    "- The ## prefix means “this is a continuation of the previous token.”\n",
    "- 'convert', 'words', 'to', 'numbers', '!' — These are standard tokens.\n",
    "- '[SEP]' — Special separator token that marks the end of a single sentence or separates multiple sentences.\n",
    "\n",
    "\n",
    "#### Input IDs: [101, 19204, 17629, 2015, 10463, 2616, 2000, 3616, 999, 102]\n",
    "\n",
    "These are the IDs that correspond to each token above. The tokenizer uses a vocabulary to map each token to a unique integer.\n",
    "- 101 is the ID for [CLS]\n",
    "- 102 is the ID for [SEP]\n",
    "- The rest are IDs for 'token', '##izer', etc., specific to BERT's vocabulary.\n",
    "\n",
    "#### Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "These are used to distinguish between multiple sentences in tasks like question answering. Since this is a single sentence input, all values are 0.\n",
    "\n",
    "If you had two segments like:\n",
    "tokenizer(\"Question?\", \"Answer.\")\n",
    "You'd get 0s for the first part, and 1s for the second.\n",
    "\n",
    "#### Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "This tells the model which tokens should be attended to (i.e., not padding).\n",
    "- 1 = real token (should be processed)\n",
    "- 0 = padding (ignore this token)\n",
    "Here, since there’s no padding, all tokens are marked 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67dccd88a6dab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 19204, 17629, 2015, 10463, 2616, 2000, 3616, 1012, 102, 2023, 2003, 2183, 2200, 3835, 1012, 102]\n",
      "Token Type IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokens: ['[CLS]', 'token', '##izer', '##s', 'convert', 'words', 'to', 'numbers', '.', '[SEP]', 'this', 'is', 'going', 'very', 'nice', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "seq1 = \"Tokenizers convert words to numbers.\"\n",
    "seq2 = \"This is going very nice.\"\n",
    "\n",
    "tokens = tokenizer(seq1, seq2)\n",
    "\n",
    "print(\"Input IDs:\", tokens[\"input_ids\"])\n",
    "print(\"Token Type IDs:\", tokens[\"token_type_ids\"])\n",
    "print(\"Attention Mask:\", tokens[\"attention_mask\"])\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08660ec8-f20c-4a74-bc0d-e3b5ec5acbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
