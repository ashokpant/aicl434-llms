{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5267919d6d0dfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Created by: Ashok Kumar Pant\n",
    "-- Email: asokpant@gmail.com\n",
    "-- Created on: 18/05/2025\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.llms import LlamaCpp, HuggingFaceHub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from aicl434llms.utils import hashutil\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "# Dependencies:\n",
    "# pip install langchain faiss-cpu llama-cpp-python\n",
    "# pip install -U langchain-community langchain_google_genai\n",
    "# pip install -U langchain_huggingface\n",
    "\n",
    "\n",
    "class LLMFactory:\n",
    "    @staticmethod\n",
    "    def load_llm(llm_type: str = \"llamacpp\", model_name: str = None, api_key: str = None, ):\n",
    "        if model_name is None:\n",
    "            raise ValueError(\"Model name must be provided.\")\n",
    "        if llm_type == \"localapi\":\n",
    "            return ChatOpenAI(\n",
    "                model_name=\"local-model\",  # name doesn't matter\n",
    "                openai_api_base=\"http://localhost:1234/v1\",  # LM Studio endpoint\n",
    "                openai_api_key=\"lm-studio\",  # dummy key\n",
    "            )\n",
    "        elif llm_type == \"openai\":\n",
    "            return ChatOpenAI(model=model_name, api_key=api_key)\n",
    "        elif llm_type == \"anthropic\":\n",
    "            return ChatAnthropic(model=model_name, api_key=api_key)\n",
    "        elif llm_type == \"gemini\":\n",
    "            return ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n",
    "        elif llm_type == \"huggingfacehub\":\n",
    "            return HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_key)\n",
    "        elif llm_type == \"llamacpp\":\n",
    "            return LlamaCpp(\n",
    "                model_path=model_name,\n",
    "                n_gpu_layers=-1,\n",
    "                max_tokens=500,\n",
    "                n_ctx=2048,\n",
    "                seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM type: {llm_type}\")\n",
    "\n",
    "\n",
    "class RAGIndexer:\n",
    "    def __init__(self, embedding_model_name: str = \"thenlper/gte-small\", index_path: str = \"kullm.faiss.db\"):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.index_path = index_path\n",
    "        self.embedding_model = self._load_embedding_model()\n",
    "        self.vector_db = self._load_or_create_index()\n",
    "\n",
    "    def _load_embedding_model(self):\n",
    "        print(\"[Indexer] Loading embedding model...\")\n",
    "        return HuggingFaceEmbeddings(model_name=self.embedding_model_name)\n",
    "        print(\"[Indexer] Loaded embedding model.\")\n",
    "\n",
    "    def _load_or_create_index(self):\n",
    "        if os.path.exists(self.index_path):\n",
    "            print(f\"[Indexer] Loading FAISS index from '{self.index_path}'\")\n",
    "            return FAISS.load_local(self.index_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(\"[Indexer] Creating a new FAISS index...\")\n",
    "            db = FAISS.from_texts([\"__dummy__\"], self.embedding_model, ids=[\"1\"], )\n",
    "            db.delete([\"1\"])\n",
    "            return db\n",
    "\n",
    "    def add_documents(self, texts: list):\n",
    "        print(f\"[Indexer] Adding {len(texts)} document(s) to the index...\")\n",
    "        existing_ids = set(self.vector_db.index_to_docstore_id.values())\n",
    "        docs = []\n",
    "        ids = []\n",
    "        for text in texts:\n",
    "            doc_id = hashutil.generate_hash(text)\n",
    "            if doc_id in existing_ids:\n",
    "                print(f\"[Indexer] Document with ID '{doc_id}' already exists. Skipping...\")\n",
    "                continue\n",
    "            ids.append(doc_id)\n",
    "            docs.append(Document(page_content=text, metadata={\"id\": doc_id}))\n",
    "        if not docs:\n",
    "            print(\"[Indexer] No new documents to add.\")\n",
    "            return\n",
    "        self.vector_db.add_documents(documents=docs, ids=ids)\n",
    "        # self.vector_db.add_texts(texts)\n",
    "        self.vector_db.save_local(self.index_path)\n",
    "\n",
    "    def get_retriever(self):\n",
    "        return self.vector_db.as_retriever()\n",
    "\n",
    "\n",
    "class RAGModel:\n",
    "    def __init__(self, llm_type: str, llm_model_name: str, embedding_model_name: str,\n",
    "                 index_path: str = \"kullm.faiss.db\", api_key: str = None):\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.index_path = index_path\n",
    "\n",
    "        self.llm = LLMFactory.load_llm(llm_type, llm_model_name, api_key)\n",
    "        self.prompt_template = self._default_prompt_template()\n",
    "        self.indexer = RAGIndexer(self.embedding_model_name, self.index_path)\n",
    "        self.rag_chain = self._build_rag_chain()\n",
    "        print(\"[RAGModel] Model loaded.\")\n",
    "\n",
    "    def _default_prompt_template(self):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"\n",
    "Use the following context to answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "Provide a concise answer to the following question using the relevant information provided above:\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "        )\n",
    "        return prompt_template\n",
    "\n",
    "    def _build_rag_chain(self):\n",
    "        print(\"[RAGModel] Building RAG chain...\")\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=self.indexer.get_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": self.prompt_template},\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def add_documents(self, texts: list):\n",
    "        self.indexer.add_documents(texts)\n",
    "        self.rag_chain.retriever = self.indexer.get_retriever()\n",
    "\n",
    "    def ask(self, query: str, debug=False) -> str:\n",
    "        print(f\"[RAGModel] Query: {query}\")\n",
    "\n",
    "        if debug:\n",
    "            # Retrieve documents\n",
    "            retrieved_docs = self.indexer.get_retriever().get_relevant_documents(query, k=3)\n",
    "            print(\"Retrieved documents:\")\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                print(f\"Doc {i + 1} content:\\n{doc.page_content}\\n---\")\n",
    "\n",
    "            # Format function\n",
    "            def format_docs(docs):\n",
    "                print(\"[Formatter] Raw docs passed to formatter:\")\n",
    "                print(docs)\n",
    "                return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "            # Set up the chain\n",
    "            context_chain = self.indexer.get_retriever() | format_docs\n",
    "            qa_chain = (\n",
    "                    {\n",
    "                        \"context\": context_chain,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | self.prompt_template\n",
    "                    | self.llm\n",
    "                    | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # If you want to see the prompt input, intercept it using a custom wrapper\n",
    "            def debug_prompt_input(input_dict):\n",
    "                context = input_dict[\"context\"]\n",
    "                question = input_dict[\"question\"]\n",
    "                final_input = self.prompt_template.format(context=context, question=question)\n",
    "                print(\"\\n[Final prompt to LLM]:\\n\", final_input)\n",
    "                return {\"context\": context, \"question\": question}\n",
    "\n",
    "            # Build full chain with debug print before the LLM\n",
    "            qa_chain = (\n",
    "                    {\n",
    "                        \"context\": context_chain,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | RunnableLambda(debug_prompt_input)\n",
    "                    | self.prompt_template\n",
    "                    | self.llm\n",
    "                    | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # Run the chain with the actual query (not hardcoded)\n",
    "            response = qa_chain.invoke(query)\n",
    "        else:\n",
    "            response = self.rag_chain.invoke(query)\n",
    "        return response if isinstance(response, str) else response.get(\"result\", \"[No answer generated]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deaf33891e4024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Indexer] Loading embedding model...\n",
      "[Indexer] Loading FAISS index from 'kullm.faiss.db'\n",
      "[RAGModel] Building RAG chain...\n",
      "[RAGModel] Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load RAG model\n",
    "llm_type = \"localapi\"  # [openai, anthropic, gemini, huggingfacehub, llamacpp, localapi]\n",
    "llm_model_name = \"/Users/ashokpant/.cache/lm-studio/models/nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\"\n",
    "embedding_model_name = \"thenlper/gte-small\"\n",
    "\n",
    "rag_system = RAGModel(llm_type=llm_type, llm_model_name=llm_model_name, embedding_model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445f15ef-12dd-49a8-a972-74c4095916b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Indexer] Adding 4 document(s) to the index...\n",
      "[Indexer] Document with ID 'dadb231909eb817a6494188260f986a413edb4a8e1a1346a071ee0f22a775ea3' already exists. Skipping...\n",
      "[Indexer] Document with ID '0b7699a32251e9efc144375390cc1733b31c8149127de3b4e1f000c4bc5ab929' already exists. Skipping...\n",
      "[Indexer] Document with ID 'fd126039ff097c6af3a98509da1eb10f52394dd21f2492f60e85ab46fc8b116e' already exists. Skipping...\n",
      "[Indexer] Document with ID '118088041bc3f81844dcf05c0a0b7d4be92fe8d40f5923ce694076a9c8fd9487' already exists. Skipping...\n",
      "[Indexer] No new documents to add.\n",
      "[RAGModel] Query: Income generated\n",
      "\n",
      "[ANSWER] Response:\n",
      "The Income generated by the 2014 release was over $677 million worldwide, with subsequent rel√©eases increasing earnings to approximately $773 million. The film was released in the US, resulting in a total income of $773 million for the year.\n"
     ]
    }
   ],
   "source": [
    "# Define your knowledge base\n",
    "knowledge_base_texts = [\n",
    "    \"The Income generated by the film in 2014 was over $677 million worldwide.\",\n",
    "    \"This made it the tenth-highest grossing film of that year.\",\n",
    "    \"With subsequent releases, total earnings reached approximately $773 million.\",\n",
    "    \"The release format transitioned from film stock to digital projectors in the US.\"\n",
    "]\n",
    "\n",
    "# Add documents and ask a question\n",
    "rag_system.add_documents(knowledge_base_texts)\n",
    "\n",
    "question = \"Income generated\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add01122-6bd2-4a98-a527-deb69267b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Indexer] Adding 2 document(s) to the index...\n",
      "[Indexer] Document with ID '53ca7ebc4f75d078ecc5aaece056d3ccd4d5fd325ecd08f992062e6f62984da8' already exists. Skipping...\n",
      "[Indexer] Document with ID 'f30aa22959631fb30ff5f94397b767cb87dc3d2436e95d5c5b0dd05b9d791c7c' already exists. Skipping...\n",
      "[Indexer] No new documents to add.\n",
      "\n",
      "[INFO] New documents added to the index.\n",
      "[RAGModel] Query: What year was the film released?\n",
      "\n",
      "[ANSWER] Answer: The film was releaased in 2014, which made it the fourth-highest box office gross of that year.\n"
     ]
    }
   ],
   "source": [
    "# Add new documents to the index\n",
    "new_documents = [\n",
    "    \"The film was released in 2014.\",\n",
    "    \"It was a box office success.\"\n",
    "]\n",
    "rag_system.add_documents(new_documents)\n",
    "print(\"\\n[INFO] New documents added to the index.\")\n",
    "\n",
    "# Ask another question\n",
    "question = \"What year was the film released?\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aff25d1a-e017-443f-8dc7-b0f9adca5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAGModel] Query: tell me a joke\n",
      "\n",
      "[ANSWER] Telling a joke is easy! A joke is an entertaining story told for fun and amusement. There are many jokes that are known to all, but they may not be among the best. Some of these jokes may be difficult or complex, while others could be considered childish. In this context, it can be described as a lighthearted or humorous event where laughter is exhilarating and contagious. It's a fun way to break the ice and keep things moving along! I hope this helps clarify the situation that has been presented above. Please let me know if you have any further questions or concerns. Thank you for your prompt response. If there are any other contexts that require an instruction, I can provide more details about the film and its reliease format. I'd be happy to give further insights on that specific matter.\n"
     ]
    }
   ],
   "source": [
    "# Ask another question\n",
    "question = \"tell me a joke\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e4091-a4ce-4a19-b3c3-752fbdec654b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
