{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "-- Created by: Ashok Kumar Pant\n",
    "-- Email: asokpant@gmail.com\n",
    "-- Created on: 18/05/2025\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.llms import LlamaCpp, HuggingFaceHub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from aicl434llms.utils import hashutil\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "# Dependencies:\n",
    "# pip install langchain faiss-cpu llama-cpp-python\n",
    "# pip install -U langchain-community langchain_google_genai\n",
    "# pip install -U langchain_huggingface\n",
    "\n",
    "\n",
    "class LLMFactory:\n",
    "    @staticmethod\n",
    "    def load_llm(llm_type: str = \"llamacpp\", model_name: str = None, api_key: str = None, ):\n",
    "        if model_name is None:\n",
    "            raise ValueError(\"Model name must be provided.\")\n",
    "        if llm_type == \"localapi\":\n",
    "            return ChatOpenAI(\n",
    "                model_name=\"local-model\",  # name doesn't matter\n",
    "                openai_api_base=\"http://localhost:1234/v1\",  # LM Studio endpoint\n",
    "                openai_api_key=\"lm-studio\",  # dummy key\n",
    "            )\n",
    "        elif llm_type == \"openai\":\n",
    "            return ChatOpenAI(model=model_name, api_key=api_key)\n",
    "        elif llm_type == \"anthropic\":\n",
    "            return ChatAnthropic(model=model_name, api_key=api_key)\n",
    "        elif llm_type == \"gemini\":\n",
    "            return ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n",
    "        elif llm_type == \"huggingfacehub\":\n",
    "            return HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_key)\n",
    "        elif llm_type == \"llamacpp\":\n",
    "            return LlamaCpp(\n",
    "                model_path=model_name,\n",
    "                n_gpu_layers=-1,\n",
    "                max_tokens=500,\n",
    "                n_ctx=2048,\n",
    "                seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM type: {llm_type}\")\n",
    "\n",
    "\n",
    "class RAGIndexer:\n",
    "    def __init__(self, embedding_model_name: str = \"thenlper/gte-small\", index_path: str = \"kullm.faiss.db\"):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.index_path = index_path\n",
    "        self.embedding_model = self._load_embedding_model()\n",
    "        self.vector_db = self._load_or_create_index()\n",
    "\n",
    "    def _load_embedding_model(self):\n",
    "        print(\"[Indexer] Loading embedding model...\")\n",
    "        return HuggingFaceEmbeddings(model_name=self.embedding_model_name)\n",
    "        print(\"[Indexer] Loaded embedding model.\")\n",
    "\n",
    "    def _load_or_create_index(self):\n",
    "        if os.path.exists(self.index_path):\n",
    "            print(f\"[Indexer] Loading FAISS index from '{self.index_path}'\")\n",
    "            return FAISS.load_local(self.index_path, self.embedding_model, allow_dangerous_deserialization=True)\n",
    "        else:\n",
    "            print(\"[Indexer] Creating a new FAISS index...\")\n",
    "            db = FAISS.from_texts([\"__dummy__\"], self.embedding_model, ids=[\"1\"], )\n",
    "            db.delete([\"1\"])\n",
    "            return db\n",
    "\n",
    "    def add_documents(self, texts: list):\n",
    "        print(f\"[Indexer] Adding {len(texts)} document(s) to the index...\")\n",
    "        existing_ids = set(self.vector_db.index_to_docstore_id.values())\n",
    "        docs = []\n",
    "        ids = []\n",
    "        for text in texts:\n",
    "            doc_id = hashutil.generate_hash(text)\n",
    "            if doc_id in existing_ids:\n",
    "                print(f\"[Indexer] Document with ID '{doc_id}' already exists. Skipping...\")\n",
    "                continue\n",
    "            ids.append(doc_id)\n",
    "            docs.append(Document(page_content=text, metadata={\"id\": doc_id}))\n",
    "        if not docs:\n",
    "            print(\"[Indexer] No new documents to add.\")\n",
    "            return\n",
    "        self.vector_db.add_documents(documents=docs, ids=ids)\n",
    "        # self.vector_db.add_texts(texts)\n",
    "        self.vector_db.save_local(self.index_path)\n",
    "\n",
    "    def get_retriever(self):\n",
    "        return self.vector_db.as_retriever()\n",
    "\n",
    "\n",
    "class RAGModel:\n",
    "    def __init__(self, llm_type: str, llm_model_name: str, embedding_model_name: str,\n",
    "                 index_path: str = \"kullm.faiss.db\", api_key: str = None):\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.index_path = index_path\n",
    "\n",
    "        self.llm = LLMFactory.load_llm(llm_type, llm_model_name, api_key)\n",
    "        self.prompt_template = self._default_prompt_template()\n",
    "        self.indexer = RAGIndexer(self.embedding_model_name, self.index_path)\n",
    "        self.rag_chain = self._build_rag_chain()\n",
    "        print(\"[RAGModel] Model loaded.\")\n",
    "\n",
    "    def _default_prompt_template(self):\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=\"\"\"\n",
    "Use the following context to answer the question.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "Provide a concise answer to the following question using the relevant information provided above:\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "        )\n",
    "        return prompt_template\n",
    "\n",
    "    def _build_rag_chain(self):\n",
    "        print(\"[RAGModel] Building RAG chain...\")\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=self.indexer.get_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": self.prompt_template},\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def add_documents(self, texts: list):\n",
    "        self.indexer.add_documents(texts)\n",
    "        self.rag_chain.retriever = self.indexer.get_retriever()\n",
    "\n",
    "    def ask(self, query: str, debug=False) -> str:\n",
    "        print(f\"[RAGModel] Query: {query}\")\n",
    "\n",
    "        if debug:\n",
    "            # Retrieve documents\n",
    "            retrieved_docs = self.indexer.get_retriever().get_relevant_documents(query, k=3)\n",
    "            print(\"Retrieved documents:\")\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                print(f\"Doc {i + 1} content:\\n{doc.page_content}\\n---\")\n",
    "\n",
    "            # Format function\n",
    "            def format_docs(docs):\n",
    "                print(\"[Formatter] Raw docs passed to formatter:\")\n",
    "                print(docs)\n",
    "                return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "            # Set up the chain\n",
    "            context_chain = self.indexer.get_retriever() | format_docs\n",
    "            qa_chain = (\n",
    "                    {\n",
    "                        \"context\": context_chain,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | self.prompt_template\n",
    "                    | self.llm\n",
    "                    | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # If you want to see the prompt input, intercept it using a custom wrapper\n",
    "            def debug_prompt_input(input_dict):\n",
    "                context = input_dict[\"context\"]\n",
    "                question = input_dict[\"question\"]\n",
    "                final_input = self.prompt_template.format(context=context, question=question)\n",
    "                print(\"\\n[Final prompt to LLM]:\\n\", final_input)\n",
    "                return {\"context\": context, \"question\": question}\n",
    "\n",
    "            # Build full chain with debug print before the LLM\n",
    "            qa_chain = (\n",
    "                    {\n",
    "                        \"context\": context_chain,\n",
    "                        \"question\": RunnablePassthrough(),\n",
    "                    }\n",
    "                    | RunnableLambda(debug_prompt_input)\n",
    "                    | self.prompt_template\n",
    "                    | self.llm\n",
    "                    | StrOutputParser()\n",
    "            )\n",
    "\n",
    "            # Run the chain with the actual query (not hardcoded)\n",
    "            response = qa_chain.invoke(query)\n",
    "        else:\n",
    "            response = self.rag_chain.invoke(query)\n",
    "        return response if isinstance(response, str) else response.get(\"result\", \"[No answer generated]\")"
   ],
   "id": "5267919d6d0dfa37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load RAG model\n",
    "llm_type = \"localapi\"  # [openai, anthropic, gemini, huggingfacehub, llamacpp, localapi]\n",
    "llm_model_name = \"/Users/ashokpant/.cache/lm-studio/models/nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\"\n",
    "embedding_model_name = \"thenlper/gte-small\"\n",
    "\n",
    "rag_system = RAGModel(llm_type=llm_type, llm_model_name=llm_model_name, embedding_model_name=embedding_model_name)"
   ],
   "id": "deaf33891e4024a"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "445f15ef-12dd-49a8-a972-74c4095916b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Indexer] Adding 4 document(s) to the index...\n",
      "[Indexer] Document with ID 'dadb231909eb817a6494188260f986a413edb4a8e1a1346a071ee0f22a775ea3' already exists. Skipping...\n",
      "[Indexer] Document with ID '0b7699a32251e9efc144375390cc1733b31c8149127de3b4e1f000c4bc5ab929' already exists. Skipping...\n",
      "[Indexer] Document with ID 'fd126039ff097c6af3a98509da1eb10f52394dd21f2492f60e85ab46fc8b116e' already exists. Skipping...\n",
      "[Indexer] Document with ID '118088041bc3f81844dcf05c0a0b7d4be92fe8d40f5923ce694076a9c8fd9487' already exists. Skipping...\n",
      "[Indexer] No new documents to add.\n",
      "[RAGModel] Query: Income generated\n",
      "\n",
      "[ANSWER] Yes, according to the context provided above, the income generated by the Income in 2014 was $677 million worldwide.\n"
     ]
    }
   ],
   "source": [
    "# Define your knowledge base\n",
    "knowledge_base_texts = [\n",
    "    \"The Income generated by the film in 2014 was over $677 million worldwide.\",\n",
    "    \"This made it the tenth-highest grossing film of that year.\",\n",
    "    \"With subsequent releases, total earnings reached approximately $773 million.\",\n",
    "    \"The release format transitioned from film stock to digital projectors in the US.\"\n",
    "]\n",
    "\n",
    "# Add documents and ask a question\n",
    "rag_system.add_documents(knowledge_base_texts)\n",
    "\n",
    "question = \"Income generated\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "add01122-6bd2-4a98-a527-deb69267b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Indexer] Adding 2 document(s) to the index...\n",
      "\n",
      "[INFO] New documents added to the index.\n",
      "[RAGModel] Query: What year was the film released?\n",
      "\n",
      "[ANSWER] Answer: 2014\n"
     ]
    }
   ],
   "source": [
    "# Add new documents to the index\n",
    "new_documents = [\n",
    "    \"The film was released in 2014.\",\n",
    "    \"It was a box office success.\"\n",
    "]\n",
    "rag_system.add_documents(new_documents)\n",
    "print(\"\\n[INFO] New documents added to the index.\")\n",
    "\n",
    "# Ask another question\n",
    "question = \"What year was the film released?\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff25d1a-e017-443f-8dc7-b0f9adca5753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
