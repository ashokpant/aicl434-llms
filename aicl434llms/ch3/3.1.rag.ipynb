{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Created by: Ashok Kumar Pant\n",
    "-- Email: asokpant@gmail.com\n",
    "-- Created on: 18/05/2025\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.llms import LlamaCpp, HuggingFaceHub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Dependencies:\n",
    "# pip install langchain faiss-cpu llama-cpp-python\n",
    "# pip install -U langchain-community langchain_google_genai\n",
    "# pip install -U langchain_huggingface\n",
    "\n",
    "\n",
    "class LLMFactory:\n",
    "    @staticmethod\n",
    "    def load_llm(llm_type: str = \"llamacpp\", model_name: str = None, api_key: str = None, ):\n",
    "        if model_name is None:\n",
    "            raise ValueError(\"Model name must be provided.\")\n",
    "        if llm_type == \"openai\":\n",
    "            return ChatOpenAI(model=model_name, api_key=api_key)\n",
    "        elif llm_type == \"anthropic\":\n",
    "            return ChatAnthropic(model=model_name, api_key=api_key)\n",
    "        elif llm_type == \"gemini\":\n",
    "            return ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n",
    "        elif llm_type == \"huggingfacehub\":\n",
    "            return HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=api_key)\n",
    "        elif llm_type == \"llamacpp\":\n",
    "            return LlamaCpp(\n",
    "                model_path=model_name,\n",
    "                n_gpu_layers=-1,\n",
    "                max_tokens=500,\n",
    "                n_ctx=2048,\n",
    "                seed=42,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported LLM type: {llm_type}\")\n",
    "\n",
    "\n",
    "class RAGIndexer:\n",
    "    def __init__(self, embedding_model_name: str = \"thenlper/gte-small\", index_path: str = \"kullm.faiss.db\"):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.index_path = index_path\n",
    "        self.embedding_model = self._load_embedding_model()\n",
    "        self.vector_db = self._load_or_create_index()\n",
    "\n",
    "    def _load_embedding_model(self):\n",
    "        print(\"[Indexer] Loading embedding model...\")\n",
    "        return HuggingFaceEmbeddings(model_name=self.embedding_model_name)\n",
    "\n",
    "    def _load_or_create_index(self):\n",
    "        if os.path.exists(self.index_path):\n",
    "            print(f\"[Indexer] Loading FAISS index from '{self.index_path}'\")\n",
    "            return FAISS.load_local(self.index_path, self.embedding_model)\n",
    "        else:\n",
    "            print(\"[Indexer] Creating a new FAISS index...\")\n",
    "            return FAISS.from_texts([\"\"], self.embedding_model)\n",
    "\n",
    "\n",
    "    def add_documents(self, texts: list):\n",
    "        print(f\"[Indexer] Adding {len(texts)} document(s) to the index...\")\n",
    "        self.vector_db.add_texts(texts)\n",
    "        self.vector_db.save_local(self.index_path)\n",
    "\n",
    "    def get_retriever(self):\n",
    "        return self.vector_db.as_retriever()\n",
    "\n",
    "\n",
    "class RAGModel:\n",
    "    def __init__(self, llm_type: str, llm_model_name: str, embedding_model_name: str,\n",
    "                 index_path: str = \"kullm.faiss.db\", api_key: str = None):\n",
    "        self.llm_model_name = llm_model_name\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.index_path = index_path\n",
    "\n",
    "        self.llm = LLMFactory.load_llm(llm_type, llm_model_name, api_key)\n",
    "        self.prompt_template = self._default_prompt_template()\n",
    "        self.indexer = RAGIndexer(self.embedding_model_name, self.index_path)\n",
    "        self.rag_chain = self._build_rag_chain()\n",
    "\n",
    "    def _load_llm(self):\n",
    "        print(\"[INFO] Loading generation model...\")\n",
    "        return LlamaCpp(\n",
    "            model_path=self.llm_model_name,\n",
    "            n_gpu_layers=-1,\n",
    "            max_tokens=500,\n",
    "            n_ctx=2048,\n",
    "            seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def _default_prompt_template(self):\n",
    "        template = \"\"\"<|user|>\n",
    "Relevant information:\n",
    "{context}\n",
    "Provide a concise answer to the following question using the relevant information provided above:\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "        return PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def _build_rag_chain(self):\n",
    "        print(\"[INFO] Building RAG chain...\")\n",
    "        return RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type='stuff',\n",
    "            retriever=self.indexer.get_retriever(),\n",
    "            chain_type_kwargs={\"prompt\": self.prompt_template},\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    def add_documents(self, texts: list):\n",
    "        self.indexer.add_documents(texts)\n",
    "        self.rag_chain.retriever = self.indexer.get_retriever()\n",
    "\n",
    "    def ask(self, query: str) -> str:\n",
    "        print(f\"[RAGModel] Query: {query}\")\n",
    "        response = self.rag_chain.invoke(query)\n",
    "        return response.get(\"result\", \"[No answer generated]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef40321a31fdf43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Indexer] Loading embedding model...\n",
      "[Indexer] Creating a new FAISS index...\n",
      "[INFO] Building RAG chain...\n"
     ]
    }
   ],
   "source": [
    "# Load RAG model\n",
    "llm_model_name = \"/Users/ashokpant/.cache/lm-studio/models/nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf\"\n",
    "embedding_model_name = \"thenlper/gte-small\"\n",
    "\n",
    "rag_system = RAGModel(llm_type=\"llamacpp\", llm_model_name=llm_model_name, embedding_model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f15ef-12dd-49a8-a972-74c4095916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your knowledge base\n",
    "knowledge_base_texts = [\n",
    "    \"The Income generated by the film in 2014 was over $677 million worldwide.\",\n",
    "    \"This made it the tenth-highest grossing film of that year.\",\n",
    "    \"With subsequent releases, total earnings reached approximately $773 million.\",\n",
    "    \"The release format transitioned from film stock to digital projectors in the US.\"\n",
    "]\n",
    "\n",
    "# Add documents and ask a question\n",
    "rag_system.add_documents(knowledge_base_texts)\n",
    "\n",
    "question = \"Income generated\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)\n",
    "\n",
    "# Add new documents to the index\n",
    "new_documents = [\n",
    "    \"The film was released in 2014.\",\n",
    "    \"It was a box office success.\"\n",
    "]\n",
    "rag_system.add_documents(new_documents)\n",
    "print(\"\\n[INFO] New documents added to the index.\")\n",
    "\n",
    "# Ask another question\n",
    "question = \"What year was the film released?\"\n",
    "answer = rag_system.ask(question)\n",
    "print(\"\\n[ANSWER]\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
