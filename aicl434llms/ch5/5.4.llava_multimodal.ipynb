{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multimodal LLMs: LLaVA-1.5\n",
    "Ashok Kumar Pant\n"
   ],
   "id": "2c8be5a7e3c686da"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T04:18:16.199768Z",
     "start_time": "2025-06-12T04:18:16.151293Z"
    }
   },
   "source": [
    "import torch\n",
    "# Install dependencies (uncomment if running on a fresh notebook)\n",
    "# !pip install transformers accelerate bitsandbytes torch torchvision --quiet\n",
    "# !pip install git+https://github.com/haotian-liu/LLaVA.git --quiet\n",
    "# !pip install peft safetensors --quiet\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-12T04:18:17.655371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"  # example repo, adjust to your actual model repo\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ").to(\"cuda\")"
   ],
   "id": "a8fa3275a89dad5a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e196df3b448642dea13fa8d6b314b845"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5344df138e4347bdb49647ea928a3389"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b838ed4cb1cd4fef8a9d13f72c24a80c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33d92349ff5b4c2faee77c7e4f9dc201"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbb3d7f237054efb900cba782fc69d0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "# Utility function\n",
    "def generate_response(image_url, prompt, chat_mode=False):\n",
    "    # Load and convert the image\n",
    "    image = Image.open(BytesIO(requests.get(image_url).content)).convert(\"RGB\")\n",
    "\n",
    "    # Process input\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
    "\n",
    "    # Generate model output\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    # Decode and clean up output\n",
    "    response = processor.batch_decode(output, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    # Additional cleanup for known special tokens (if any appear)\n",
    "    for token in [\"<|start_of_text|>\", \"<|end_of_text|>\", \"<|user|>\", \"<|assistant|>\"]:\n",
    "        response = response.replace(token, \"\").strip()\n",
    "\n",
    "    return response\n"
   ],
   "id": "3e44f5f05f9110b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------------------------------\n",
    "# Image Captioning\n",
    "# --------------------------------------\n",
    "caption_prompt = \"Describe this image in detail.\"\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n",
    "caption = generate_response(image_url, caption_prompt)\n",
    "print(\"Image Caption:\\n\", caption)"
   ],
   "id": "68fe45e3470994f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------------------------------\n",
    "# Visual Question Answering\n",
    "# --------------------------------------\n",
    "vqa_prompt = \"What is the man doing in the image?\"\n",
    "vqa_answer = generate_response(image_url, vqa_prompt)\n",
    "print(\"VQA Answer:\\n\", vqa_answer)"
   ],
   "id": "322a24be8a4f2c3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --------------------------------------\n",
    "# Multimodal Dialogue\n",
    "# --------------------------------------\n",
    "dialogue_prompt = \"\"\"USER: What are the people doing in the image?\n",
    "ASSISTANT: They are sitting at a table having food.\n",
    "\n",
    "USER: What kind of food do you see?\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "multimodal_response = generate_response(image_url, dialogue_prompt, chat_mode=True)\n",
    "print(\"Multimodal Dialogue Response:\\n\", multimodal_response)"
   ],
   "id": "e85826cd35ec6122"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
