{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f40093-1ecf-46da-903d-1d88737079a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.3912010149105054\n",
      "Epoch 2/100, Loss: 1.3911923302976534\n",
      "Epoch 3/100, Loss: 1.391183603048437\n",
      "Epoch 4/100, Loss: 1.3911748296171274\n",
      "Epoch 5/100, Loss: 1.391166006441113\n",
      "Epoch 6/100, Loss: 1.3911571299394703\n",
      "Epoch 7/100, Loss: 1.391148196511523\n",
      "Epoch 8/100, Loss: 1.3911392025354008\n",
      "Epoch 9/100, Loss: 1.3911301443665833\n",
      "Epoch 10/100, Loss: 1.3911210183364418\n",
      "Epoch 11/100, Loss: 1.391111820750769\n",
      "Epoch 12/100, Loss: 1.391102547888299\n",
      "Epoch 13/100, Loss: 1.3910931959992188\n",
      "Epoch 14/100, Loss: 1.3910837613036693\n",
      "Epoch 15/100, Loss: 1.3910742399902323\n",
      "Epoch 16/100, Loss: 1.3910646282144086\n",
      "Epoch 17/100, Loss: 1.3910549220970843\n",
      "Epoch 18/100, Loss: 1.3910451177229795\n",
      "Epoch 19/100, Loss: 1.3910352111390893\n",
      "Epoch 20/100, Loss: 1.3910251983531063\n",
      "Epoch 21/100, Loss: 1.3910150753318282\n",
      "Epoch 22/100, Loss: 1.3910048379995559\n",
      "Epoch 23/100, Loss: 1.3909944822364655\n",
      "Epoch 24/100, Loss: 1.3909840038769725\n",
      "Epoch 25/100, Loss: 1.3909733987080752\n",
      "Epoch 26/100, Loss: 1.390962662467678\n",
      "Epoch 27/100, Loss: 1.3909517908429008\n",
      "Epoch 28/100, Loss: 1.390940779468364\n",
      "Epoch 29/100, Loss: 1.3909296239244577\n",
      "Epoch 30/100, Loss: 1.390918319735588\n",
      "Epoch 31/100, Loss: 1.3909068623684024\n",
      "Epoch 32/100, Loss: 1.3908952472299931\n",
      "Epoch 33/100, Loss: 1.390883469666079\n",
      "Epoch 34/100, Loss: 1.3908715249591617\n",
      "Epoch 35/100, Loss: 1.3908594083266599\n",
      "Epoch 36/100, Loss: 1.3908471149190174\n",
      "Epoch 37/100, Loss: 1.3908346398177862\n",
      "Epoch 38/100, Loss: 1.3908219780336832\n",
      "Epoch 39/100, Loss: 1.3908091245046208\n",
      "Epoch 40/100, Loss: 1.3907960740937075\n",
      "Epoch 41/100, Loss: 1.3907828215872224\n",
      "Epoch 42/100, Loss: 1.3907693616925587\n",
      "Epoch 43/100, Loss: 1.3907556890361383\n",
      "Epoch 44/100, Loss: 1.3907417981612946\n",
      "Epoch 45/100, Loss: 1.3907276835261246\n",
      "Epoch 46/100, Loss: 1.390713339501308\n",
      "Epoch 47/100, Loss: 1.3906987603678922\n",
      "Epoch 48/100, Loss: 1.390683940315045\n",
      "Epoch 49/100, Loss: 1.3906688734377701\n",
      "Epoch 50/100, Loss: 1.3906535537345897\n",
      "Epoch 51/100, Loss: 1.3906379751051872\n",
      "Epoch 52/100, Loss: 1.390622131348015\n",
      "Epoch 53/100, Loss: 1.3906060161578608\n",
      "Epoch 54/100, Loss: 1.3905896231233785\n",
      "Epoch 55/100, Loss: 1.3905729457245755\n",
      "Epoch 56/100, Loss: 1.3905559773302603\n",
      "Epoch 57/100, Loss: 1.3905387111954475\n",
      "Epoch 58/100, Loss: 1.390521140458719\n",
      "Epoch 59/100, Loss: 1.3905032581395431\n",
      "Epoch 60/100, Loss: 1.3904850571355465\n",
      "Epoch 61/100, Loss: 1.390466530219741\n",
      "Epoch 62/100, Loss: 1.3904476700377026\n",
      "Epoch 63/100, Loss: 1.390428469104704\n",
      "Epoch 64/100, Loss: 1.3904089198027951\n",
      "Epoch 65/100, Loss: 1.3903890143778366\n",
      "Epoch 66/100, Loss: 1.390368744936479\n",
      "Epoch 67/100, Loss: 1.390348103443092\n",
      "Epoch 68/100, Loss: 1.390327081716638\n",
      "Epoch 69/100, Loss: 1.3903056714274924\n",
      "Epoch 70/100, Loss: 1.390283864094208\n",
      "Epoch 71/100, Loss: 1.3902616510802208\n",
      "Epoch 72/100, Loss: 1.3902390235905004\n",
      "Epoch 73/100, Loss: 1.3902159726681385\n",
      "Epoch 74/100, Loss: 1.3901924891908777\n",
      "Epoch 75/100, Loss: 1.3901685638675791\n",
      "Epoch 76/100, Loss: 1.3901441872346256\n",
      "Epoch 77/100, Loss: 1.3901193496522624\n",
      "Epoch 78/100, Loss: 1.3900940413008693\n",
      "Epoch 79/100, Loss: 1.3900682521771692\n",
      "Epoch 80/100, Loss: 1.3900419720903665\n",
      "Epoch 81/100, Loss: 1.3900151906582154\n",
      "Epoch 82/100, Loss: 1.3899878973030182\n",
      "Epoch 83/100, Loss: 1.38996008124755\n",
      "Epoch 84/100, Loss: 1.389931731510913\n",
      "Epoch 85/100, Loss: 1.3899028369043092\n",
      "Epoch 86/100, Loss: 1.3898733860267427\n",
      "Epoch 87/100, Loss: 1.389843367260641\n",
      "Epoch 88/100, Loss: 1.389812768767395\n",
      "Epoch 89/100, Loss: 1.3897815784828207\n",
      "Epoch 90/100, Loss: 1.3897497841125375\n",
      "Epoch 91/100, Loss: 1.3897173731272603\n",
      "Epoch 92/100, Loss: 1.3896843327580075\n",
      "Epoch 93/100, Loss: 1.3896506499912205\n",
      "Epoch 94/100, Loss: 1.3896163115637954\n",
      "Epoch 95/100, Loss: 1.3895813039580216\n",
      "Epoch 96/100, Loss: 1.3895456133964295\n",
      "Epoch 97/100, Loss: 1.3895092258365436\n",
      "Epoch 98/100, Loss: 1.3894721269655395\n",
      "Epoch 99/100, Loss: 1.3894343021948035\n",
      "Epoch 100/100, Loss: 1.3893957366543925\n",
      "Test output: [[0.49966578]\n",
      " [0.50033422]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ElmanRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize the parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.input_size) * 0.01  # Input to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.Why = np.random.randn(self.output_size, self.hidden_size) * 0.01  # Hidden to output\n",
    "        self.bh = np.zeros((self.hidden_size, 1))  # Hidden bias\n",
    "        self.by = np.zeros((self.output_size, 1))  # Output bias\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        self.h_prev = np.zeros((self.hidden_size, 1))  # Previous hidden state\n",
    "        self.x = x\n",
    "        self.z = np.dot(self.Wxh, self.x) + np.dot(self.Whh, self.h_prev) + self.bh\n",
    "        self.h = self.tanh(self.z)\n",
    "        self.y = np.dot(self.Why, self.h) + self.by\n",
    "        self.output = self.softmax(self.y)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y_true):\n",
    "        # Backpropagation to compute gradients\n",
    "        self.dL_dy = self.output - y_true\n",
    "        self.dL_dWhy = np.dot(self.dL_dy, self.h.T)\n",
    "        self.dL_dby = self.dL_dy\n",
    "        \n",
    "        self.dL_dh = np.dot(self.Why.T, self.dL_dy)\n",
    "        self.dL_dz = self.dL_dh * self.tanh_derivative(self.z)\n",
    "        self.dL_dWhh = np.dot(self.dL_dz, self.h_prev.T)\n",
    "        self.dL_dWxh = np.dot(self.dL_dz, self.x.T)\n",
    "        self.dL_dbh = self.dL_dz\n",
    "\n",
    "        # Gradients for previous hidden state\n",
    "        self.dL_dh_prev = np.dot(self.Whh.T, self.dL_dz)\n",
    "        \n",
    "        return self.dL_dWxh, self.dL_dWhh, self.dL_dWhy, self.dL_dbh, self.dL_dby, self.dL_dh_prev\n",
    "\n",
    "    def update_parameters(self, grads):\n",
    "        # Update the parameters using the gradients and learning rate\n",
    "        dL_dWxh, dL_dWhh, dL_dWhy, dL_dbh, dL_dby, _ = grads\n",
    "        self.Wxh -= self.learning_rate * dL_dWxh\n",
    "        self.Whh -= self.learning_rate * dL_dWhh\n",
    "        self.Why -= self.learning_rate * dL_dWhy\n",
    "        self.bh -= self.learning_rate * dL_dbh\n",
    "        self.by -= self.learning_rate * dL_dby\n",
    "\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(len(X)):\n",
    "                # Forward pass\n",
    "                output = self.forward(X[i])\n",
    "\n",
    "                # Compute loss (using Cross-Entropy for classification)\n",
    "                loss = -np.sum(y[i] * np.log(output))\n",
    "                total_loss += loss\n",
    "\n",
    "                # Backward pass and parameter updates\n",
    "                grads = self.backward(y[i])\n",
    "                self.update_parameters(grads)\n",
    "\n",
    "            # Print the loss for the current epoch\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Toy dataset: X (input sequence) and y (target sequence)\n",
    "    X = [np.array([[0], [1]]), np.array([[1], [0]])]  # input sequences\n",
    "    y = [np.array([[1], [0]]), np.array([[0], [1]])]  # target sequences\n",
    "\n",
    "    # Create an Elman RNN\n",
    "    rnn = ElmanRNN(input_size=2, hidden_size=3, output_size=2, learning_rate=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    rnn.train(X, y, epochs=100)\n",
    "\n",
    "    # Testing with a new input\n",
    "    test_input = np.array([[.1], [0.9]])\n",
    "    output = rnn.forward(test_input)\n",
    "    print(\"Test output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7978e6-4f0a-4420-9fe0-b5bc2253ccb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
