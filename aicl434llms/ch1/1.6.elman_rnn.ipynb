{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28391de7c3917400",
   "metadata": {},
   "source": [
    "# Elman RNN\n",
    "Ashok Kumar Pant | AI Solution Architect | CTO and Co-founder at Treeleaf/Anydone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742382a8fc0eba3",
   "metadata": {},
   "source": [
    "**1. Introduction**\n",
    "Elman RNN is a type of recurrent neural network (RNN) proposed by Jeffrey Elman in 1990. It is one of the simplest RNN architectures and is often used to model sequential data.\n",
    "\n",
    "**2. Architecture of Elman RNN**\n",
    "\n",
    "Elman RNN consists of three layers:\n",
    "\n",
    "- **Input Layer $x_t$** - Takes in sequential data at each time step.\n",
    "- **Hidden Layer $h_t$** - Has **recurrent connections**, meaning it receives inputs from both:\n",
    "   - The **current input** $x_t$\n",
    "   - The **previous hidden state** $h_{t-1}$\n",
    "- **Output Layer $y_t$** - Produces the network's output at each time step.\n",
    "\n",
    "The **key feature** of Elman RNN is the presence of **context units**, which store past information and help in sequential learning.\n",
    "\n",
    "**3. Mathematical Formulation**\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "- **Hidden state update**:\n",
    "   \n",
    "   $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$\n",
    "\n",
    "   where:\n",
    "   - $W_{xh} $ = Weight matrix for input to hidden layer\n",
    "   - $W_{hh} $ = Weight matrix for hidden-to-hidden recurrence\n",
    "   - $b_h $ = Bias term\n",
    "   - $f $ = Activation function (e.g., tanh or ReLU)\n",
    "\n",
    "- **Output computation**:\n",
    "   \n",
    "   $y_t = g(W_{hy} h_t + b_y)$\n",
    "   where:\n",
    "   - $W_{hy} $ = Weight matrix from hidden to output\n",
    "   - $b_y $ = Bias term\n",
    "   - $g $ = Activation function (e.g., softmax for classification)\n",
    "\n",
    "- **Loss function (for training using Backpropagation Through Time - BPTT)**:\n",
    "  \n",
    "   $L = \\sum_{t=1}^{T} \\mathcal{L}(y_t, \\hat{y}_t)$\n",
    "   where $\\mathcal{L} $ is the loss function (e.g., cross-entropy for classification, mean squared error for regression).\n",
    "\n",
    "- **Gradient computation (for weight updates using BPTT)**:\n",
    "\n",
    "   $\\frac{\\partial L}{\\partial W_{xh}}, \\frac{\\partial L}{\\partial W_{hh}}, \\frac{\\partial L}{\\partial W_{hy}}$\n",
    "   These gradients are calculated by unrolling the network in time and applying the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85462c436aedf5c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ElmanRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ElmanRNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        for t in range(inputs.size(1)):  # Iterate over sequence length\n",
    "            hidden = self.rnn_cell(inputs[:, t, :], hidden)\n",
    "        output = self.fc(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        return torch.zeros(batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class RNNModelTrainer:\n",
    "    def __init__(self, input_size=1, hidden_size=10, output_size=1, sequence_length=5, batch_size=10, num_epochs=200,\n",
    "                 lr=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = ElmanRNNModel(input_size, hidden_size, output_size).to(self.device)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, inputs, targets):\n",
    "        self.model.train() # Set the model to training mode\n",
    "        inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "        dataset_size = len(inputs)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for i in range(0, dataset_size, self.batch_size):\n",
    "                batch_inputs = inputs[i:i + self.batch_size]\n",
    "                batch_targets = targets[i:i + self.batch_size]\n",
    "                batch_size = batch_inputs.shape[0]\n",
    "                batch_hidden = self.model.init_hidden(batch_size, self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output, _ = self.model(batch_inputs, batch_hidden)\n",
    "                loss = self.criterion(output, batch_targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {epoch_loss / (dataset_size // self.batch_size):.4f}')\n",
    "\n",
    "    def save_model(self, path='model.bin'):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'input_size': self.input_size,\n",
    "            'output_size': self.output_size,\n",
    "            'sequence_length': self.sequence_length\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path='model.bin'):\n",
    "        checkpoint = torch.load(path)\n",
    "        self = cls()\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.sequence_length = checkpoint['sequence_length']\n",
    "        self.input_size = checkpoint['input_size']\n",
    "        self.output_size = checkpoint['output_size']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return self\n",
    "\n",
    "    def infer(self, input_sequence):\n",
    "        self.model.eval() # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            input_sequence = input_sequence.to(self.device)\n",
    "            batch_size = input_sequence.shape[0]\n",
    "            hidden = self.model.init_hidden(batch_size, self.device)\n",
    "            output, _ = self.model(input_sequence, hidden)\n",
    "        return output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fd64ec6c81d736",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, eq=\"x**2\", num_samples=1000, sequence_length=5):\n",
    "        self.eq = eq\n",
    "        self.num_samples = num_samples\n",
    "        self.sequence_length = sequence_length\n",
    "        self.x_values, self.y_values = self.generate_data()\n",
    "        self.input_dim = 1\n",
    "        self.output_dim = 1\n",
    "\n",
    "    def generate_data(self):\n",
    "        X = range(self.num_samples)\n",
    "        Y = [eval(self.eq, {\"x\": x}) for x in X]\n",
    "        x_values = np.array(X, dtype=np.float32).reshape(-1, 1)\n",
    "        y_values = np.array(Y, dtype=np.float32).reshape(-1, 1)\n",
    "        return x_values, y_values\n",
    "\n",
    "    def prepare_data(self):\n",
    "        inputs = np.array(\n",
    "            [self.x_values[i:i + self.sequence_length] for i in range(len(self.x_values) - self.sequence_length)],\n",
    "            dtype=np.float32)\n",
    "        targets = np.array(\n",
    "            [self.y_values[i + self.sequence_length] for i in range(len(self.y_values) - self.sequence_length)],\n",
    "            dtype=np.float32)\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)  # Shape: (batch_size, seq_len, input_size)\n",
    "        targets = torch.tensor(targets, dtype=torch.float32).reshape(-1, 1)  # Shape: (batch_size, output_size)\n",
    "        return inputs, targets\n",
    "\n",
    "    def generate_input_sequence(self, x):\n",
    "        \"\"\"Generate a sequence of numbers to be used as input for inference.\"\"\"\n",
    "        x_values = np.array([x - self.sequence_length + i + 1 for i in range(self.sequence_length)],\n",
    "                            dtype=np.float32).reshape(1, self.sequence_length, self.input_dim)\n",
    "        return torch.tensor(x_values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f5bdab0ccb37e9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (Input -> Target):\n",
      "Input: [tensor([0.]), tensor([1.]), tensor([2.])] -> Target: [tensor(7.)]\n",
      "Input: [tensor([1.]), tensor([2.]), tensor([3.])] -> Target: [tensor(9.)]\n",
      "Input: [tensor([2.]), tensor([3.]), tensor([4.])] -> Target: [tensor(11.)]\n",
      "Input: [tensor([3.]), tensor([4.]), tensor([5.])] -> Target: [tensor(13.)]\n",
      "Input: [tensor([4.]), tensor([5.]), tensor([6.])] -> Target: [tensor(15.)]\n",
      "Input: [tensor([5.]), tensor([6.]), tensor([7.])] -> Target: [tensor(17.)]\n",
      "Input: [tensor([6.]), tensor([7.]), tensor([8.])] -> Target: [tensor(19.)]\n",
      "Input: [tensor([7.]), tensor([8.]), tensor([9.])] -> Target: [tensor(21.)]\n",
      "Input: [tensor([8.]), tensor([9.]), tensor([10.])] -> Target: [tensor(23.)]\n",
      "Input: [tensor([9.]), tensor([10.]), tensor([11.])] -> Target: [tensor(25.)]\n"
     ]
    }
   ],
   "source": [
    "# Explore dataset\n",
    "d = Dataset(eq=\"2*x+1\", num_samples=100, sequence_length=3)\n",
    "x,y = d.prepare_data()\n",
    "print(\"Training Data (Input -> Target):\")\n",
    "for i in range(min(10, len(x))):\n",
    "    print(f\"Input: {list(x[i])} -> Target: {list(y[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610f31ba6bbd0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 230107832875.1633\n",
      "Epoch 20, Loss: 230068437014.3377\n",
      "Epoch 40, Loss: 230029939044.1391\n",
      "Epoch 60, Loss: 229991530483.3286\n",
      "Epoch 80, Loss: 229953145283.8508\n",
      "Epoch 100, Loss: 229914777111.4753\n",
      "Epoch 120, Loss: 229876427701.5670\n",
      "Epoch 140, Loss: 229838081461.1220\n",
      "Epoch 160, Loss: 229799757704.1593\n",
      "Epoch 180, Loss: 229761430635.7896\n",
      "Epoch 200, Loss: 229723119812.0096\n",
      "Epoch 220, Loss: 229684896921.6326\n",
      "Epoch 240, Loss: 229646511451.4000\n",
      "Epoch 260, Loss: 229608230779.5515\n",
      "Epoch 280, Loss: 229569945025.5795\n",
      "Epoch 300, Loss: 229531673080.3809\n",
      "Epoch 320, Loss: 229493405073.0851\n",
      "Epoch 340, Loss: 229455152433.7860\n",
      "Epoch 360, Loss: 229416896395.1168\n",
      "Epoch 380, Loss: 229378659062.9243\n",
      "Epoch 400, Loss: 229340416548.1173\n",
      "Epoch 420, Loss: 229302185908.5614\n",
      "Epoch 440, Loss: 229263969219.6527\n",
      "Epoch 460, Loss: 229225758506.0052\n",
      "Epoch 480, Loss: 229187555442.3069\n",
      "Epoch 500, Loss: 229149367272.9478\n",
      "Epoch 520, Loss: 229111173912.3839\n",
      "Epoch 540, Loss: 229072993874.9788\n",
      "Epoch 560, Loss: 229034826147.4732\n",
      "Epoch 580, Loss: 228996667732.8821\n",
      "Epoch 600, Loss: 228958509608.7029\n",
      "Epoch 620, Loss: 228920366125.2425\n",
      "Epoch 640, Loss: 228882223069.8899\n",
      "Epoch 660, Loss: 228844095653.1570\n",
      "Epoch 680, Loss: 228805964027.0840\n",
      "Epoch 700, Loss: 228767849643.2489\n",
      "Epoch 720, Loss: 228729732827.8104\n",
      "Epoch 740, Loss: 228691643593.8928\n",
      "Epoch 760, Loss: 228653550481.4580\n",
      "Epoch 780, Loss: 228615463551.8694\n",
      "Epoch 800, Loss: 228577385759.8130\n",
      "Epoch 820, Loss: 228539321367.0663\n",
      "Epoch 840, Loss: 228501265644.2509\n",
      "Epoch 860, Loss: 228463211399.9266\n",
      "Epoch 880, Loss: 228425172284.0566\n",
      "Epoch 900, Loss: 228387135734.9958\n",
      "Epoch 920, Loss: 228349117988.1288\n",
      "Epoch 940, Loss: 228311101840.0593\n",
      "Epoch 960, Loss: 228273087003.1967\n",
      "Epoch 980, Loss: 228235072570.5154\n",
      "Model saved to elmanrnn.bin\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(num_samples=1000, sequence_length=5)\n",
    "model = RNNModelTrainer(input_size=dataset.input_dim, hidden_size=10, output_size=dataset.output_dim,\n",
    "                 sequence_length=dataset.sequence_length, batch_size=32, num_epochs=1000, lr=0.01)\n",
    "inputs, targets = dataset.prepare_data()\n",
    "\n",
    "model.train(inputs, targets)\n",
    "model.save_model('elmanrnn.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53115acaf55f4d47",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from elmanrnn.bin\n",
      "Input Sequence: [46.0, 47.0, 48.0, 49.0, 50.0]\n",
      "Predicted Output: 2622.9868\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "model = RNNModelTrainer.load_model(path='elmanrnn.bin')\n",
    "dataset = Dataset(sequence_length=model.sequence_length)\n",
    "test_seq = dataset.generate_input_sequence(50)\n",
    "predicted_output = model.infer(test_seq)\n",
    "\n",
    "print(f\"Input Sequence: {test_seq.flatten().tolist()}\")\n",
    "print(f\"Predicted Output: {predicted_output.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29701be589bfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
