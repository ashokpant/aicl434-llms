{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28391de7c3917400",
   "metadata": {},
   "source": [
    "# Elman RNN\n",
    "Ashok Kumar Pant | AI Solution Architect | CTO and Co-founder at Treeleaf/Anydone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742382a8fc0eba3",
   "metadata": {},
   "source": [
    "**1. Introduction**\n",
    "Elman RNN is a type of recurrent neural network (RNN) proposed by Jeffrey Elman in 1990. It is one of the simplest RNN architectures and is often used to model sequential data.\n",
    "\n",
    "**2. Architecture of Elman RNN**\n",
    "\n",
    "Elman RNN consists of three layers:\n",
    "\n",
    "- **Input Layer $x_t$** - Takes in sequential data at each time step.\n",
    "- **Hidden Layer $h_t$** - Has **recurrent connections**, meaning it receives inputs from both:\n",
    "   - The **current input** $x_t$\n",
    "   - The **previous hidden state** $h_{t-1}$\n",
    "- **Output Layer $y_t$** - Produces the network's output at each time step.\n",
    "\n",
    "The **key feature** of Elman RNN is the presence of **context units**, which store past information and help in sequential learning.\n",
    "\n",
    "**3. Mathematical Formulation**\n",
    "\n",
    "At each time step $t$:\n",
    "\n",
    "- **Hidden state update**:\n",
    "   \n",
    "   $h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$\n",
    "\n",
    "   where:\n",
    "   - $W_{xh} $ = Weight matrix for input to hidden layer\n",
    "   - $W_{hh} $ = Weight matrix for hidden-to-hidden recurrence\n",
    "   - $b_h $ = Bias term\n",
    "   - $f $ = Activation function (e.g., tanh or ReLU)\n",
    "\n",
    "- **Output computation**:\n",
    "   \n",
    "   $y_t = g(W_{hy} h_t + b_y)$\n",
    "   where:\n",
    "   - $W_{hy} $ = Weight matrix from hidden to output\n",
    "   - $b_y $ = Bias term\n",
    "   - $g $ = Activation function (e.g., softmax for classification)\n",
    "\n",
    "- **Loss function (for training using Backpropagation Through Time - BPTT)**:\n",
    "  \n",
    "   $L = \\sum_{t=1}^{T} \\mathcal{L}(y_t, \\hat{y}_t)$\n",
    "   where $\\mathcal{L} $ is the loss function (e.g., cross-entropy for classification, mean squared error for regression).\n",
    "\n",
    "- **Gradient computation (for weight updates using BPTT)**:\n",
    "\n",
    "   $\\frac{\\partial L}{\\partial W_{xh}}, \\frac{\\partial L}{\\partial W_{hh}}, \\frac{\\partial L}{\\partial W_{hy}}$\n",
    "   These gradients are calculated by unrolling the network in time and applying the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b8a8ee6210ebcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        out, h = self.rnn(x, h)\n",
    "        out = self.fc(out[:, -1, :])  # Use the last time step's output\n",
    "        return out, h\n",
    "\n",
    "# Example Usage\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 3\n",
    "\n",
    "rnn = ElmanRNN(input_size, hidden_size, output_size)\n",
    "x = torch.randn(1, 4, input_size)  # Batch=1, SeqLen=4, Features=5\n",
    "h = torch.zeros(1, 1, hidden_size) # Initial hidden state\n",
    "\n",
    "output, h = rnn(x, h)\n",
    "print(output.shape)  # (1, 3) -> Output for the last time step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81e3acc-1a31-44e9-ba17-40434f545579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    \"\"\"An Elman RNN built using RNNCell.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): Size of the input vectors.\n",
    "            hidden_size (int): Size of the hidden state vectors.\n",
    "            batch_first (bool): Whether the 0th dimension is batch.\n",
    "        \"\"\"\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def initialize_hidden(self, batch_size):\n",
    "        \"\"\"Initializes the hidden state with zeros.\"\"\"\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the ElmanRNN.\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): Input data tensor.\n",
    "                If self.batch_first: x_in.shape = (batch_size, seq_size, feat_size)\n",
    "                Else: x_in.shape = (seq_size, batch_size, feat_size)\n",
    "            initial_hidden (torch.Tensor, optional): Initial hidden state for the RNN.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The outputs of the RNN at each time step.\n",
    "                If self.batch_first: shape = (batch_size, seq_size, hidden_size)\n",
    "                Else: shape = (seq_size, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)  # Convert to (seq_size, batch_size, feat_size)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        \n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self.initialize_hidden(batch_size)\n",
    "        \n",
    "        initial_hidden = initial_hidden.to(x_in.device)\n",
    "        hidden_t = initial_hidden\n",
    "\n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)  # Convert back to (batch_size, seq_size, hidden_size)\n",
    "\n",
    "        return hiddens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90432741-29a5-4f19-b23a-7c2b99fdd958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f219e5-162a-4a06-9ed0-21f3b1cebe1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "RNNCell: Expected hidden to be 1D or 2D, got 3D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, input_size)  \u001b[38;5;66;03m# Batch=1, SeqLen=4, Features=5\u001b[39;00m\n\u001b[1;32m      8\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, hidden_size) \u001b[38;5;66;03m# Initial hidden state\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output, h \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (1, 3) -> Output for the last time step\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mElmanRNN.forward\u001b[0;34m(self, x_in, initial_hidden)\u001b[0m\n\u001b[1;32m     50\u001b[0m hidden_t \u001b[38;5;241m=\u001b[39m initial_hidden\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_size):\n\u001b[0;32m---> 53\u001b[0m     hidden_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     hiddens\u001b[38;5;241m.\u001b[39mappend(hidden_t)\n\u001b[1;32m     56\u001b[0m hiddens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(hiddens)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1568\u001b[0m, in \u001b[0;36mRNNCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNNCell: Expected input to be 1D or 2D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1566\u001b[0m     )\n\u001b[1;32m   1567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m-> 1568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNNCell: Expected hidden to be 1D or 2D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n",
      "\u001b[0;31mValueError\u001b[0m: RNNCell: Expected hidden to be 1D or 2D, got 3D instead"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 3\n",
    "\n",
    "rnn = ElmanRNN(input_size, hidden_size, output_size)\n",
    "x = torch.randn(1, 4, input_size)  # Batch=1, SeqLen=4, Features=5\n",
    "h = torch.zeros(1, 1, hidden_size) # Initial hidden state\n",
    "\n",
    "output, h = rnn(x, h)\n",
    "print(output.shape)  # (1, 3) -> Output for the last time step\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dade042-02d3-48de-a9bc-577d98dbc48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sklearn/utils/_encode.py:235\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sklearn/utils/_encode.py:174\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    173\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sklearn/utils/_encode.py:167\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 6",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnationality\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnationality\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSurnameDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SurnameDataset(test_data, label_encoder)\n\u001b[1;32m     42\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mSurnameDataset.__init__\u001b[0;34m(self, df, label_encoder, seq_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, label_encoder, seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurnames \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnationalities \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnationality\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length \u001b[38;5;241m=\u001b[39m seq_length\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:134\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.12/site-packages/sklearn/utils/_encode.py:237\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 6"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load dataset (assumed structure: \"name, nationality\")\n",
    "df = pd.read_csv('../../data/names-by-nationality.csv')\n",
    "\n",
    "# Preprocess: Convert surnames to characters\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder, seq_length=15):\n",
    "        self.surnames = df['name'].values\n",
    "        self.nationalities = label_encoder.transform(df['nationality'].values)\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.surnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        surname = self.surnames[idx]\n",
    "        nationality = self.nationalities[idx]\n",
    "        \n",
    "        # Pad surname to sequence length (or truncate if too long)\n",
    "        surname = surname[:self.seq_length].ljust(self.seq_length)\n",
    "        \n",
    "        # Convert surname characters to ASCII values\n",
    "        surname_chars = torch.tensor([ord(c) for c in surname], dtype=torch.long)\n",
    "        \n",
    "        return surname_chars, nationality\n",
    "\n",
    "# Prepare data\n",
    "label_encoder = LabelEncoder()\n",
    "df['nationality'] = label_encoder.fit_transform(df['nationality'])\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SurnameDataset(train_data, label_encoder)\n",
    "test_dataset = SurnameDataset(test_data, label_encoder)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the Elman RNN model\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size=256, hidden_size=128, output_size=10, seq_length=15):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding layer for input characters (if needed, here we're using raw ASCII values)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # Elman RNN layer\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get embeddings for each character\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # RNN layer\n",
    "        rnn_out, _ = self.rnn(x)  # rnn_out shape: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # We only care about the output of the last character (the last timestep)\n",
    "        last_hidden_state = rnn_out[:, -1, :]\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 256  # ASCII range (0-255)\n",
    "hidden_size = 128\n",
    "output_size = len(label_encoder.classes_)  # Number of nationalities\n",
    "seq_length = 15  # Max length of surname\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = ElmanRNN(input_size, hidden_size, output_size, seq_length)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for surname_chars, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(surname_chars)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for surname_chars, labels in test_loader:\n",
    "        outputs = model(surname_chars)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d7855-e3f7-495e-96bc-633e3bae2efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
